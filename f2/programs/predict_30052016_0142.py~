from scipy import sparse
from sklearn import cross_validation
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.decomposition import IncrementalPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble.bagging import BaggingClassifier, BaggingRegressor
from sklearn.ensemble.forest import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.ensemble.voting_classifier import VotingClassifier
from sklearn.externals import joblib
from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV
from sklearn.linear_model.coordinate_descent import ElasticNetCV, LassoCV
from sklearn.linear_model.ridge import RidgeCV, RidgeClassifier, RidgeClassifierCV
from sklearn.linear_model.stochastic_gradient import SGDClassifier, SGDRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.preprocessing import Binarizer, FunctionTransformer, Imputer, LabelBinarizer, LabelEncoder, MaxAbsScaler, MinMaxScaler, OneHotEncoder, RobustScaler, StandardScaler
from sklearn.svm import LinearSVR, NuSVC, NuSVR, OneClassSVM, SVC, SVR
from sklearn_pandas import DataFrameMapper
from sklearn2pmml.decoration import CategoricalDomain, ContinuousDomain
from pandas import DataFrame
from xgboost.sklearn import XGBClassifier, XGBRegressor

# for PCA
import sklearn.preprocessing, sklearn.decomposition

import numpy
import pandas



SEED = 42  # always use a seed for randomized procedures

def load_csv(name):
	return pandas.read_csv("data/" + name, na_values = ["N/A", "NA"])

def store_csv(df, name):
	df.to_csv("csv/" + name, sep=";", index = False)

def store_pkl(obj, name):
	joblib.dump(obj, "pkl/" + name, compress = 9)

def mape(sample_y, preds): 
    #y_true, y_pred = check_array(y_true, y_pred)
    return numpy.mean(numpy.abs((sample_y - preds) / sample_y)) * 100

def build_sample(regressor, name):
	# repeat the CV procedure 5 times to get more precise results
	n = 5  
	# for each iteration, randomly hold out 20% of the data as CV set
	for i in range(n):
		X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(
		      train_X, train_y, test_size=.20, random_state=i*SEED)
		# train...
		regressor = regressor.fit(X_train, y_train)
		# save model
		#store_pkl(regressor, name + ".pkl")
		# predict on train
		preds = regressor.predict(X_cv)
		# print 
		#print preds
		# create DataFrame
		#preds = DataFrame(preds, columns = ["prime_tot_ttc_preds"])
		#print preds
		#print y_cv
		# mape
		mape_r = mape(y_cv, preds)
		# print
		print "MAPE of (fold %d/%d) of %s is : %f" % (i+1 , n, name, mape_r)
	# re-train
	regressor = regressor.fit(train_X, train_y)
	# predict
	preds_on_dev = regressor.predict(dev_X)
	# mape
	mape_rr = mape(dev_y, preds_on_dev)
	# print
	print "MAPE on dev %s : %f" % (name, mape_rr) 
	# to 
	d = {"id":dev_id, "real":dev_y, "preds":preds_on_dev}
	global_result = DataFrame(d)
	store_csv(global_result, name + "_real_and_preds_on_dev.csv")
	# predict on test
	preds_on_test = regressor.predict(test_X)
	# FRame
	preds_on_test_frame = DataFrame(list(zip(test_id, preds_on_test)), columns = ["ID", "COTIS"])
	# save predictions
	store_csv(preds_on_test_frame, name + ".csv")
	return preds_on_test


print "GOOOOOOOOOO...."

sample = load_csv("sample1.csv")

#print sample.dtypes

sample['id'].astype(int)
sample['age'] = 2016 - sample['annee_naissance']
sample['duree_permis'] = 2016 - sample['annee_permis']
sample['codepostal'].astype(str)

mapper = DataFrameMapper([
	(["id"], None),
	(["crm"], MinMaxScaler()),
	([
	"age", 
	"duree_permis", 
	"var1",
	"var2",
	"var4",
	"var17",
	"var18",
	"var19",
	"var22",
	"var9",
	"var10",
	"var11",
	"var12",
	"var13",
	#"kmage_annuel", replaced by km1, km2, km3
	"puis_fiscale",
	"anc_veh",
	"var15"],
	[ContinuousDomain()]),
	([
	"km1",
	"km2",
	"km3",
	"var3",
	"var5"],[Binarizer()]),
	([ 
	"var20",
	"var21"], 
	 [Binarizer()]), 
	(["codepostal"], [LabelEncoder()]),
	(["marque"], LabelEncoder()),
	(["var14"], LabelEncoder()),
	(["energie_veh"], LabelEncoder()), 
        (["profession"], LabelEncoder()), 
        (["var6"], LabelEncoder()),
        (["var7"], LabelEncoder()),
        (["var8"], LabelEncoder()),
        (["var16"], LabelEncoder()),
	(["prime_tot_ttc"], None)
])


sample_mapper = mapper.fit_transform(sample)

print(sample_mapper)

c_size = sample_mapper.shape[1]
#print c_size

#store_pkl(mapper, "mapper.pkl")

l_train = 200000
l_dev = 300000
l_test = 330000

# train
s_train = 0
train_X = sample_mapper[s_train:l_train, 1:c_size - 1]
train_y = sample_mapper[s_train:l_train, c_size - 1]

# dev
s_dev = l_train
dev_X =  sample_mapper[s_dev:l_dev, 1:c_size - 1]
dev_y =  sample_mapper[s_dev:l_dev, c_size - 1]
dev_id = sample_mapper[s_dev:l_dev, 0]

# test
s_test = l_dev
test_X = sample_mapper[s_test:l_test, 1:c_size - 1]
test_id = sample_mapper[s_test:l_test, 0]

#print sample_X.shape
#print sample_y.shape
#print sample_t.shape


#predict_res_1 = build_sample(DecisionTreeRegressor(random_state = 0, min_samples_leaf = 2), "DecisionTreeRegressor")
#predict_res_2 = build_sample(ExtraTreesRegressor(random_state = 0, min_samples_leaf = 5), "ExtraTreesRegressor")
#predict_res_3 = build_sample(GradientBoostingRegressor(random_state = 0, init = None), "GradientBoostingRegressor")
predict_res_4 = build_sample(RandomForestRegressor(n_estimators=300, random_state = None, min_samples_split = 10, max_depth=25), "RandomForestRegressor")
#predict_res_5 = build_sample(XGBRegressor(objective = "reg:linear"), "XGBRegressor")

d = {"id":test_id, 
    #"DecisionTreeRegressor":predict_res_1, 
    #"ExtraTreesRegressor":predict_res_2, 
    #"GradientBoostingRegressor":predict_res_3, 
    "RandomForestRegressor":predict_res_4}, 
    #"XGBRegressor":predict_res_5}

global_result = DataFrame(d)

store_csv(global_result, "global_result.csv")

#print(auto_X.dtype, auto_y.dtype)
#build_sample(DecisionTreeRegressor(random_state = 13, min_samples_leaf = 5), "DecisionTreeAuto")
#build_sample(BaggingRegressor(DecisionTreeRegressor(random_state = 13, min_samples_leaf = 5), random_state = 13, n_estimators = 3, max_features = 0.5), "DecisionTreeEnsembleAuto")
#build_sample(ElasticNetCV(random_state = 13), "ElasticNetAuto")
#build_sample(LassoCV(random_state = 13), "LassoAuto")
#build_sample(LinearRegression(), "LinearRegressionAuto")
#build_sample(BaggingRegressor(LinearRegression(), random_state = 13, max_features = 0.5), "LinearRegressionEnsembleAuto")
#build_sample(RidgeCV(), "RidgeAuto")
#build_sample(XGBRegressor(objective = "reg:linear"), "XGBAuto")
